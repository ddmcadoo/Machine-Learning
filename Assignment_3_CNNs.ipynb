{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01375c32",
   "metadata": {
    "id": "01375c32"
   },
   "source": [
    "# Assignment 3: Convolutional Neural Networks\n",
    "\n",
    "## DTSC-680: Applied Machine Learning\n",
    "\n",
    "## Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249b051",
   "metadata": {
    "id": "b249b051"
   },
   "source": [
    "### Overview\n",
    "\n",
    "The [`CIFAR10` dataset](https://keras.io/api/datasets/cifar10/) contains 50,000 32 $\\times$ 32 colored images of the things listed below. \n",
    "\n",
    "| Label    | Description |\n",
    "| -------- | -------|\n",
    "| 0 | airplane    |\n",
    "| 1 | automobile     |\n",
    "| 2 | bird    |\n",
    "| 3 | cat    |\n",
    "| 4 | deer    |\n",
    "| 5 | dog    |\n",
    "| 6 | frog    |\n",
    "| 7 | horse    |\n",
    "| 8 | ship    |\n",
    "| 9 | truck    |\n",
    "\n",
    "The `base_model` contains a CNN that was trained on over 17,000 CIFAR10 images, none of which contained a horse, ship, or truck. In this assignment, you will use the bottleneck layers of the `base_model`, along with transfer learning to build a network that can classify all 10 categories of the images in the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce8065-0981-41dc-b06e-1e52b8a974c8",
   "metadata": {},
   "source": [
    "### Install Tensorflow 2.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f940fab-8cc5-49ab-a18c-f589c92bdf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/envs/dtsc680/lib/python3.8/site-packages (24.2)\n",
      "Pip updated!\n",
      "TensorFlow version 2.9.2 is already installed.\n",
      "Please restart your kernel to apply the changes.\n"
     ]
    }
   ],
   "source": [
    "# CODE PROVIDED\n",
    "import subprocess\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "def update_libraries():\n",
    "    tf_desired_version = \"2.9.2\"\n",
    "\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf_installed_version = tf.__version__\n",
    "    except ImportError:\n",
    "        tf_installed_version = None\n",
    "\n",
    "    # Update pip to the latest version\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip'])\n",
    "    print(\"Pip updated!\")\n",
    "    \n",
    "    #Check for the right version\n",
    "    if tf_installed_version != tf_desired_version:\n",
    "        print(f\"Current TensorFlow version: {tf_installed_version}. Installing version {tf_desired_version}...\")\n",
    "        \n",
    "        # Uninstall the current TensorFlow version\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'uninstall', '-y', 'tensorflow'])\n",
    "        \n",
    "        # Install the desired TensorFlow version\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', f'tensorflow=={tf_desired_version}'])\n",
    "\n",
    "        clear_output()\n",
    "        print(\"TensorFlow version {tf_desired_version} installed successfully\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"TensorFlow version {tf_desired_version} is already installed.\")    \n",
    "\n",
    "\n",
    "    print(f\"Please restart your kernel to apply the changes.\")\n",
    "    \n",
    "update_libraries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671e354a",
   "metadata": {
    "id": "671e354a"
   },
   "outputs": [],
   "source": [
    "# common imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824bea7e",
   "metadata": {
    "id": "824bea7e"
   },
   "source": [
    "### Data and Model Prep\n",
    "\n",
    "Load the `base_model` and then run the following cell to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d50001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 09:22:53.787924: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# STUDENTS CODE\n",
    "from tensorflow.keras.models import load_model\n",
    "base_model = load_model('base_model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e99f1237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32516,)\n",
      "(6512,)\n"
     ]
    }
   ],
   "source": [
    "# CODE PROVIDED -- takes almost 3 minutes to run\n",
    "x_train = np.loadtxt('data/x_train.txt').reshape(32516, 32, 32, 3)\n",
    "y_train = np.loadtxt('data/y_train.txt')\n",
    "x_test = np.loadtxt('data/x_test.txt').reshape(6512, 32, 32, 3)\n",
    "y_test = np.loadtxt('data/y_test.txt')\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e5e73",
   "metadata": {},
   "source": [
    "Let's test the `base_model`. The below code loads and displays a sample image of a ship from the test dataset. Use the `base_model` to make a prediction about the object contained in the sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ec8309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9d65ef2880>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CODE PROVIDED\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "x = x_test[0]\n",
    "\n",
    "print(y_test[0])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1221f816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 296ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.07818075e-01, 8.92128527e-01, 4.90114835e-05, 3.93905611e-06,\n",
       "        2.26635095e-07, 1.69193953e-07, 2.22662488e-09]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE PROVIDED\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# preprocess the image\n",
    "x = image.img_to_array(x_test[0])\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# STUDENTS CODE\n",
    "y = base_model.predict(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0ffe1",
   "metadata": {},
   "source": [
    "What does the `base_model` predict that the sample image is? Let's use transfer learning to create a domain-specific model that can classify images of horses, ships, and trucks in addition to the rest of the images that the `base_model` was trained on.\n",
    "\n",
    "The below code extracts the bottleneck layers from the `base_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a989df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE PROVIDED\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Resizing\n",
    "model = Model(inputs = base_model.layers[0].input, outputs = base_model.layers[3].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aeeb92",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "Create the classification layers for transfer learning and train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d67c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017/1017 [==============================] - 11s 10ms/step\n",
      "204/204 [==============================] - 2s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# STUDENTS CODE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "x_train = base_model.predict(x_train)\n",
    "x_test = base_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42f0e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENTS CODE\n",
    "#Running all images for feature extraction\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "#Creating classification layers\n",
    "new_model = Sequential()\n",
    "new_model.add(Dense(256, activation='relu',input_shape=(7,)))\n",
    "new_model.add(Dense(10, activation='softmax'))\n",
    "new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a60561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3252/3252 [==============================] - 25s 8ms/step - loss: 1.3005 - accuracy: 0.5038 - val_loss: 1.3070 - val_accuracy: 0.5006\n",
      "Epoch 2/10\n",
      "3252/3252 [==============================] - 24s 7ms/step - loss: 1.2989 - accuracy: 0.5034 - val_loss: 1.3143 - val_accuracy: 0.4937\n",
      "Epoch 3/10\n",
      "3252/3252 [==============================] - 24s 7ms/step - loss: 1.2974 - accuracy: 0.5051 - val_loss: 1.3128 - val_accuracy: 0.4995\n",
      "Epoch 4/10\n",
      "3252/3252 [==============================] - 24s 7ms/step - loss: 1.2955 - accuracy: 0.5053 - val_loss: 1.3087 - val_accuracy: 0.5008\n",
      "Epoch 5/10\n",
      "3252/3252 [==============================] - 25s 8ms/step - loss: 1.2935 - accuracy: 0.5083 - val_loss: 1.3034 - val_accuracy: 0.4983\n",
      "Epoch 6/10\n",
      "3252/3252 [==============================] - 25s 8ms/step - loss: 1.2917 - accuracy: 0.5070 - val_loss: 1.3094 - val_accuracy: 0.4974\n",
      "Epoch 7/10\n",
      "2606/3252 [=======================>......] - ETA: 1:07 - loss: 1.2939 - accuracy: 0.5072"
     ]
    }
   ],
   "source": [
    "# STUDENTS CODE -- takes about 2 minutes to run\n",
    "hist = new_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f0fb7-cf58-4812-913e-e93f14136c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c9aa0",
   "metadata": {},
   "source": [
    "Plot the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b409b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STUDENTS CODE\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, ':', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab116c",
   "metadata": {},
   "source": [
    "Check to ensure the new model correctly classifies the sample image of a ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf42c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "x = x_test[0]\n",
    "\n",
    "x = image.img_to_array(x_test[0])\n",
    "x = np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e56e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENTS CODE\n",
    "y = base_model.predict(x)\n",
    "prediction = new_model.predict(y)\n",
    "\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f'{label}: {predictions[0][i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc89645-8f5f-4a74-bc22-2573bdb63d91",
   "metadata": {},
   "source": [
    "### Export Models for codegrade evaluation\n",
    "\n",
    "Export Classification and CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#export your classification model\n",
    "classification.save('classification_model.h5')\n",
    "\n",
    "#export your CNN model\n",
    "model.save('CNN_model.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dtsc680",
   "language": "python",
   "name": "dtsc680"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
